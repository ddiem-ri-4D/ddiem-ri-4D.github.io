---
title: "Note for Support Vector Machines (SVMs)"
categories:
  - Machine Learning
tags:
  - Machine Learning
toc: true
comments: true
header:
  teaser: "/assets/images/header/lambda_01.jpg"
---

Support Vector Machines hay còn được biết với tên Việt Hóa là Máy hỗ trợ vector là một thuật toán phân lớp mạnh mẽ, thuật toán này ban đầu được tìm ra bởi Vladimir N. Vapnik và dạng chuẩn hiện nay sử dụng lề mềm được tìm ra bởi Vapnik và Corinna Cortes năm 1995. Trong bài viết này, chủ yếu là trình các bước chính của thuật toán, có sử dụng nhiều nguồn tài liệu tham khảo.

Thuật toán SVM có mục đích là xây dựng một siêu phẳng hoặc một tập hợp các siêu phẳng trong một không gian nhiều chiều hoặc vô hạn chiều, để phân loại tốt nhất thì các siêu phẳng nằm ở càng xa các điểm dữ liệu của tất cả các lớp (gọi là lề - margin) càng tốt.

Đầu vào thuật toán:

$$
\mathcal{D} = \{(x_i, y_i) | x_i \in \mathbb{R}^{p}, y_i \in \{-1, 1\}\}_{i=1}^{n}
$$

Đầu ra thuật toán: hệ số $\mathbf{w}$ và $b$ của siêu phẳng tối ưu.

Các bước thực hiện của giải thuật phân lớp dùng SVM

**Bước 01**: Chọn kernel function:

1) Linear kernel: $\mathbf{K}(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i^T\mathbf{x}_j$

2) Polynomial kernel: $\mathbf{K}(\mathbf{x}_i, \mathbf{x}_j) = (1 +  \mathbf{x}_i^T\mathbf{x}_j)^p$

3) Gaussian (Radial-Basis Function - RBF) kernel:

$$
\begin{gather*}
		\mathbf{K}(\mathbf{x}_i, \mathbf{x}_j) = exp\left(-\frac{||\mathbf{x}_i - \mathbf{x}_j||^2}{2\sigma^2}\right)
\end{gather*}
$$

4) Sigmoid:

$$
\begin{gather*}
		\mathbf{K}(\mathbf{x}_i, \mathbf{x}_j) = \text{tanh}(\beta_0\mathbf{x}_i^T\mathbf{x}_j + \beta_1)
\end{gather*}
$$

**Bước 02**: Chọn một giá trị cho $C$

Không gian để so sánh các siêu phẳng (hyperplane)

Xem xét phương trình siêu phẳng $w \cdotp x + b = 0$. Nếu một điểm dữ liệu có tọa độ $(x, y)$ trong không gian nằm trên siêu phẳng này, nó thõa mãn phương trình $w \cdotp x + b = 0$. Nếu một điểm dữ liệu không nằm trên siêu phẳng, giá trị của biểu thức $w \cdotp x + b$ có thể âm hoặc dương. Để biết được điểm dữ liệu nào nằm gần siêu phẳng nhất, ta có thể tính toán $\beta = \|\ w \cdotp x + b\|\$

Cho tập dữ liệu $\mathcal{D} = \{(x_i, y_i) \|\ x_i \in \mathbb{R}^n, y_i \in \{-1, 1\}\}_{i=1}^m$, với mỗi phần tử trong tập huấn luyên, tính toán được $\beta$, gọi B là giá trị nhỏ nhất của $\beta$

$$B = \underset{i = \{1...m\}}{\text{ min }}|w\cdotp x+b|$$

Nếu ta có $k$ hyperplane, mỗi hyperplane có một giá trị $B_i$ tương ứng, ta chỉ cần chọn hyperplane với giá trị $B_i$ lớn nhất

$$H = \underset{i = \{1...k\}}{\text{ max }}\{h_i | B_i\}$$

Vấn đề ở đây là ta có thể thất bại trong việc phân tách một siêu phẳng tốt với một siêu phẳng không tốt, vì có thể chúng có cùng giá trị.

Do đó, ta sử dụng thông tin về nhãn $y$. Định nghĩa hàm $f = y(w.x + b)$ và dấu của hàm $f$ cho biết điểm dữ liệu được phân đúng lớp (dấu dương) và phân sai lớp (dấu âm)

Dùng tập dữ liệu $\mathcal{D} = \{(x_i, y_i) \|\ x_i \in \mathbb{R}^n, y_i \in \{-1, 1\}\}_{i=1}^m$, với mỗi phần tử trong tập huấn luyên, tính toán $f$ và đặt $F$ là giá trị $f$ nhỏ nhất

$$F = \underset{i = \{1...m\}}{\text{ min }}y_i(w \cdot x+b)$$

Nếu ta có $k$ hyperplane, hyperplane nào có giá trị $F$ lớn nhất sẽ được chọn ($F$ được gọi là function margin)

Nếu ta có những vectors có cùng vector cơ sở, ví dụ như $w_1 = (5, 6)$ và $w_2 = (50, 60)$, nó không bất biến với phép co dãn (scale).

Để khắc phục yếu điểm đó, ta chia $f$ với độ lớn vector $w$. Ta định nghĩa được:

$$\gamma = y (\frac{w}{\left\| w\right\|} \cdotp x +\frac{b}{\left\| w\right\|})$$

Dùng tập dữ liệu $\mathcal{D} = \{(x_i, y_i) \|\ x_i \in \mathbb{R}^n, y_i \in \{-1, 1\}\}_{i=1}^m$, với mỗi phần tử trong tập huấn luyện, tính toán được $\gamma$ và ta gọi $M$ là giá trị nhỏ nhất, $M$ được gọi là geometric margin của tập dữ liệu

$$
  M = \underset{i = \{1...m\}}{min}y_i\left(\frac{w}{||w||} \cdot x +\frac{b}{||w||} \right)
$$

Nếu ta có $k$ hyperplane, ta sẽ chọn hyperplane với một giá trị $M$ lớn nhất.

Bài toán của chúng ta là tối ưu siêu phẳng, tức là cần tìm giá trị $w$ và $b$ của một siêu phẳng tối ưu, với ràng buộc geometric margin của mỗi phần tử phải lớn hơn hoặc bằng $M$:

$$
  \begin{gather*}
    \underset{w, b}{\text{ max }}M \\
    \text{subject to } \gamma_i \geq M, i = 1...m \\
  \end{gather*}
$$

Do $M = F/\left\| w\right\|$. Ta có thể viết lại bài toán ràng buộc như sau:

$$
  \begin{gather*}
    \underset{w, b}{\text{ max }}M \\
    \text{subject to } f_i \geq F, i = 1...m \\
  \end{gather*}
 $$

Khi co dãn $w$ và $b$, bài toán không thay đổi do đó:

$$
  \begin{gather*}
    \underset{w, b}{\text{ max }}\frac{1}{\left\| W\right\|} \\
    \text{subject to } f_i \geq 1, i = 1...m \\
  \end{gather*}
$$

$$
  \begin{gather*}
    \Leftrightarrow \underset{w, b}{\text{ min }}\left\| w\right\| \\
    \text{subject to } f_i \geq 1, i = 1...m \\
  \end{gather*}
$$

$$
  \begin{gather*}
    \Leftrightarrow \underset{w, b}{\text{ min }}\frac{1}{2}\left\| w\right\|^2 \\
    \text{subject to } y_i(w \cdotp x+b) -1 \geq 0, i = 1...m \\
  \end{gather*}
$$

**Giải bài toán tối ưu SVM (SVM optimization problem) - Hard Margin SVM**

Lagrange phát biểu rằng nếu chúng ta tìm cực tiểu hàm $f$ dưới một ràng buộc $g$, chúng ta chỉ cần giải:

$$
  \begin{gather*}
    \nabla f(x) - \alpha\nabla g(x) = 0
  \end{gather*}
$$

Trong đó: $\alpha$ được gọi là nhân tử Lagrange (Lagrange multiplier)

Với bài toán này, ta có hàm $f(w) = \frac{1}{2}\left\| w\right\|^2$, $g(w, b) = y_i(w \cdotp x + b - 1, i=1...m$. Hàm Lagrange được khai triển như sau:

$$
  \begin{gather*}
    \mathcal{L}(w, b, \alpha) = \frac{1}{2}\left\| w\right\|^2 - \sum_{i=1}^m\alpha_i[y_i(w \cdotp x + b - 1] 
  \end{gather*}
$$

Để giải phương trình $\nabla \mathcal{L}(w, b, \alpha) = 0$, ta viết lại bài toán theo nguyên lý đối ngẫu (duality principle)

$$
\begin{gather*}
    \underset{w, b}{\text{ min }}\text{ max } \mathcal{L}(w, b, \alpha)\\
    \text{subject to } \alpha_i \geq F, i = 1...m \\
  \end{gather*}
$$

Một cách chính xác hơn, $\alpha$ nên là nhân tử Karush-Kuhn-Tucker bởi vì ta phải vấn đề ràng buộc bất công bằng.

Với Lagrangian function:

$$
  \begin{gather*}
    \mathcal{L}(w, b, \alpha) = \frac{1}{2}||w||^2 - \sum_{i=1}^m\alpha_i[y_i(w \cdot x + b - 1]
  \end{gather*}
$$

Ta có:

$$
  \begin{gather*}
    \nabla_w\mathcal{L}(w, b, \alpha) = w - \sum_{i=1}^m\alpha_iy_ix_i = 0
  \end{gather*}
$$

$$
  \begin{gather*}
    \nabla_b\mathcal{L}(w, b, \alpha) = - \sum_{i=1}^m\alpha_iy_i = 0
  \end{gather*}
$$

Từ hai phương trình ở trên, ta có $w = \sum_{i=1}^m\alpha_iy_ix_i$ và $\sum_{i=1}^m\alpha_iy_i = 0$. Thay vào hàm Lagrangian

$$
  \begin{gather*}
    W(\alpha, b) = \sum_{i=1}^m\alpha_i - \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_j y_i y_j x_i \cdot x_j
  \end{gather*}
$$

Bài toán được viết lại như sau:

$$
  \begin{gather*}
    \underset{\alpha}{\text{ max }}\sum_{i=1}^m\alpha_i - \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_j y_i y_j x_i \cdot x_j\\
    \text{subject to } \alpha_i \geq 0, i = 1...m,  \sum_{i=1}^m\alpha_iy_i = 0\\
  \end{gather*}
$$

Mở rộng nhân tử Lagrange thành điều kiện (Karush-Kuhn-Tucker):

$$
  \begin{gather*}
    \alpha_i[y_i (w \cdot x^{*} + b) - 1] = 0
  \end{gather*}
$$

Trong đó, $x^{\*}$ là điểm mà đạt được tối ưu, $\alpha$ là giá trị tri dương cho những điểm tối ưu này, và có giá trị gần về 0 đối với những điểm còn lại. Do đó mà, $y_i(w \cdotp x^{\*} + b) - 1$ phải bằng 0, các điểm này gọi là support vectors, gần nhất với siêu phẳng.

Và ta cần tính toán $w$ và $b$ để đưa ra quyết định về siêu phẳng tối ưu.

$$
  \begin{gather*}
    w - \sum_{i=1}^m\alpha_iy_ix_i = 0\\
    \Leftrightarrow w = \sum_{i=1}^m\alpha_iy_ix_i
  \end{gather*}
$$

Để tính toán giá trị $b$:

$$
  \begin{gather*}
    y_i(w \cdot x^{*} + b) - 1 = 0\\
    \Leftrightarrow y_i^2(w \cdot x^{*} + b) - y_i = 0\\
    \Leftrightarrow b = y_i - w \cdot x^{*}
  \end{gather*}
$$

Lưu ý: $y_i^2 = 1 \forall i$

Và giá trị $b$ cũng có thể được tính:

$$
  \begin{gather*}
    b = \frac{1}{S}\sum_{i=1}^S(y_i - w \cdot x)
  \end{gather*}
$$

**Bước 04: Xây dựng hàm phân lớp từ những support vectors**

Bộ phân lớp được định nghĩa để đưa ra dự đoán. Hypothesis function h:

$$
  \begin{gather*}
    h(x_i) = \begin{cases}+1, w \cdot x + b \geq 0\\ -1,  w \cdot x + b < 0\end{cases}
  \end{gather*}
$$

**Giải bài toán tối ưu SVM (SVM optimization problem) - Soft Margin SVM**

Bài toán Hard Margin SVM giải quyết tốt cho dữ liệu phân hoạch tuyến tính. Tuy nhiên, điều này dường như hiếm gặp trong thực tế. Hầu hết các trường hợp dữ liệu sẽ chứa nhiễu và có thể không phân hoạch tuyến tính (phân bố dạng cầu, ...)

Soft Margin SVM sử dụng một trick rất đơn giản, thêm vào những biến chùng (slack variables) $\varsigma_i$ vào ràng buộc của bài toán tối ưu:

$$
  \begin{gather*}
    y_i(w \cdot x_i + b) \geq 1 - \varsigma_i, i =1...m
  \end{gather*}
$$

Nhờ sử dụng những biến chùng trong ràng buộc, khi cực tiểu hàm mục tiêu, nó có khả năng thõa mãn ràng buộc cho dù dữ liệu huấn luyện không thõa ràng buộc ban đầu. Và sử dụng L1 Regularization để giải quyết vấn đề của những giá trị biến chùng lớn. Ta có bài toán tối ưu như sau:

$$
  \begin{gather*}
    \Leftrightarrow \underset{w, b}{\text{ min }}\frac{1}{2}||w||^2 + \sum_{i=1}^m\varsigma_i\\
    \text{subject to } y_i(w.x+b) \geq 1 - \varsigma_i, i = 1...m \\
  \end{gather*}
$$

Và để tránh việc phân lớp sai trong mỗi huấn luyện, ta thêm vào ràng buộc biến chùng không âm và tham số $C$ gọi là regularization parameter để xác định giá trị biến chùng $\varsigma$ quan trọng như thế nào?

$$
  \begin{gather*}
    \Leftrightarrow \underset{w, b}{\text{ min }}\frac{1}{2}||w||^2 + C\sum_{i=1}^m\varsigma_i\\
    \text{subject to } y_i(w.x+b) \geq 1 - \varsigma_i, \varsigma_i \geq 0, i = 1...m \\
  \end{gather*}
$$

Và tiếp tục sử dụng nhân tử Lagrange, đưa bài toán về bài toán kép:

$$
  \begin{gather*}
    \underset{\alpha}{\text{ max }}\sum_{i=1}^m\alpha_i - \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_j y_i y_j x_i \cdot x_j\\
    \text{subject to } 0 \leq \alpha_i \leq C, i = 1...m,  \sum_{i=1}^m\alpha_iy_i = 0\\
  \end{gather*}
$$
